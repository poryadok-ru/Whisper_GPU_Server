# Whisper GPU Server

GPU-—Å–µ—Ä–≤–µ—Ä —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (ASR) –Ω–∞ –±–∞–∑–µ OpenAI Whisper –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ –∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∑–≤–æ–Ω–∫–æ–≤.

## üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **–í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è** —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º faster-whisper
- **–ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU** (CUDA) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
- **REST API** –Ω–∞ –±–∞–∑–µ FastAPI
- **–ú–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π** Whisper (–æ—Ç tiny –¥–æ large-v3)
- **–ì–∏–±–∫–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏** —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ (—è–∑—ã–∫, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, beam size –∏ –¥—Ä.)
- **Docker –ø–æ–¥–¥–µ—Ä–∂–∫–∞** –¥–ª—è –ª–µ–≥–∫–æ–≥–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è

## üìã –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.11+
- CUDA 12.2+ (–¥–ª—è GPU) –∏–ª–∏ CPU
- Docker (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏–∏)
- NVIDIA GPU —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è production)

## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –õ–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

1. –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:
```bash
git clone <repository-url>
cd Whisper_GPU_Server
```

2. –°–æ–∑–¥–∞–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:
```bash
python3.11 -m venv .venv
source .venv/bin/activate  # –ù–∞ Windows: .venv\Scripts\activate
```

3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
```bash
pip install -r requirements.txt
```

4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä:
```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### Docker —É—Å—Ç–∞–Ω–æ–≤–∫–∞

1. –°–æ–±–µ—Ä–∏—Ç–µ –æ–±—Ä–∞–∑:
```bash
docker build -t whisper-gpu-server .
```

2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å GPU:
```bash
docker run --gpus all -p 8000:8000 whisper-gpu-server
```

–ò–ª–∏ —Å docker-compose (—Å–æ–∑–¥–∞–π—Ç–µ `docker-compose.yml`):
```yaml
version: '3.8'
services:
  whisper:
    build: .
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

## üìö API –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞ –ø–æ –∞–¥—Ä–µ—Å–∞–º:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **OpenAPI JSON**: http://localhost:8000/openapi.json

## üîå API Endpoints

### 1. Health Check

–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞.

**GET** `/health`

**–û—Ç–≤–µ—Ç:**
```json
{
  "status": true,
  "message": "–°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç. –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: True"
}
```

### 2. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–∏

–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–∏ Whisper.

**POST** `/set_settings_model`

**–¢–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "model": "large-v3",
  "device": "cuda",
  "device_index": 0,
  "compute_type": "float16",
  "cpu_threads": 4,
  "num_workers": 1
}
```

**–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:**
- `tiny`, `tiny.en`
- `base`, `base.en`
- `small`, `small.en`
- `medium`, `medium.en`
- `large-v1`, `large-v2`, `large-v3`
- `large-v3-turbo`, `turbo`

**–£—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:**
- `cpu` - CPU –æ–±—Ä–∞–±–æ—Ç–∫–∞
- `cuda` - NVIDIA GPU
- `mps` - Apple Silicon GPU

**–¢–∏–ø—ã –≤—ã—á–∏—Å–ª–µ–Ω–∏–π:**
- `default` - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä
- `float16` - –¥–ª—è GPU
- `float32` - –¥–ª—è CPU
- `int8` - –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å

### 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∞–∑–æ–≤—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–∏

–ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.

**POST** `/set_base_settings_model`

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥–µ–ª—å `base` –Ω–∞ CPU (–∏–ª–∏ –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤ –∫–æ–¥–µ –Ω–∞ GPU).

### 4. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏

–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ (—è–∑—ã–∫, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –∏ –¥—Ä.).

**POST** `/set_settings_transcription`

**–¢–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "language": "ru",
  "task": "transcribe",
  "temperature": 0.0,
  "beam_size": 5,
  "word_timestamps": false,
  "vad_filter": true
}
```

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `language` - –∫–æ–¥ —è–∑—ã–∫–∞ (ru, en, de –∏ –¥—Ä.) –∏–ª–∏ `null` –¥–ª—è –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
- `task` - `transcribe` (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è) –∏–ª–∏ `translate` (–ø–µ—Ä–µ–≤–æ–¥ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
- `temperature` - —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0.0-1.0 –∏–ª–∏ –º–∞—Å—Å–∏–≤)
- `beam_size` - —Ä–∞–∑–º–µ—Ä –ª—É—á–∞ –ø–æ–∏—Å–∫–∞ (1-10)
- `word_timestamps` - –≤–∫–ª—é—á–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –¥–ª—è —Å–ª–æ–≤
- `vad_filter` - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –±–µ–∑–∑–≤—É—á–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤

### 5. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∞–∑–æ–≤—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏

–ë—ã—Å—Ç—Ä–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Å —Ä—É—Å—Å–∫–∏–º —è–∑—ã–∫–æ–º.

**POST** `/set_base_settings_transcription`

### 6. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ

–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤.

**POST** `/transcribe_audio`

**–§–æ—Ä–º–∞—Ç –∑–∞–ø—Ä–æ—Å–∞:** `multipart/form-data`

**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `file` - –∞—É–¥–∏–æ—Ñ–∞–π–ª (WAV, MP3, M4A, FLAC –∏ –¥—Ä.)

**–û—Ç–≤–µ—Ç:**
```json
{
  "status": true,
  "message": "–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞",
  "data": {
    "text": "–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏",
    "segments": [
      {
        "start": 0.0,
        "end": 2.5,
        "text": "–§—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞"
      }
    ],
    "language": "ru",
    "language_probability": 0.99
  }
}
```

## üí° –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### cURL

```bash
# 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è
curl http://localhost:8000/health

# 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
curl -X POST http://localhost:8000/set_base_settings_model \
  -H "Content-Type: application/json"

# 3. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
curl -X POST http://localhost:8000/set_base_settings_transcription \
  -H "Content-Type: application/json"

# 4. –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∞—É–¥–∏–æ
curl -X POST http://localhost:8000/transcribe_audio \
  -F "file=@audio.wav"
```

### Python

```python
import requests

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
response = requests.post(
    "http://localhost:8000/set_base_settings_model"
)
print(response.json())

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏
response = requests.post(
    "http://localhost:8000/set_base_settings_transcription"
)
print(response.json())

# –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
with open("audio.wav", "rb") as f:
    response = requests.post(
        "http://localhost:8000/transcribe_audio",
        files={"file": f}
    )
    result = response.json()
    print(result["data"]["text"])
```

### JavaScript/TypeScript

```typescript
// –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
await fetch('http://localhost:8000/set_base_settings_model', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' }
});

// –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è
const formData = new FormData();
formData.append('file', audioFile);

const response = await fetch('http://localhost:8000/transcribe_audio', {
  method: 'POST',
  body: formData
});

const result = await response.json();
console.log(result.data.text);
```

## üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
Whisper_GPU_Server/
‚îú‚îÄ‚îÄ main.py              # FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏ endpoints
‚îú‚îÄ‚îÄ whisper.py           # –ö–ª–∞—Å—Å FasterWhisperModel –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
‚îú‚îÄ‚îÄ models.py            # Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤/–æ—Ç–≤–µ—Ç–æ–≤
‚îú‚îÄ‚îÄ requirements.txt     # Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
‚îú‚îÄ‚îÄ Dockerfile           # Docker –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚îú‚îÄ‚îÄ README.md           # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îî‚îÄ‚îÄ test_transcribe.py  # –¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç
```

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫

–í —Ñ–∞–π–ª–µ `main.py` –º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –±–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:

```python
# –í —Ñ—É–Ω–∫—Ü–∏–∏ set_base_settings_model()
await model_instance.update_settings(
    model=EnumModels.large_v3,  # –ò–∑–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å
    device=EnumDevices.cuda,     # –ò–∑–º–µ–Ω–∏—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    compute_type=EnumComputeTypes.float16,
    cpu_threads=4,
    num_workers=1
)
```

## üêõ –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –Ω–µ–ø–æ–ª–∞–¥–æ–∫

### –ú–æ–¥–µ–ª—å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è

1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞:
```bash
curl -X POST http://localhost:8000/set_base_settings_model
```

2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ç—É—Å:
```bash
curl http://localhost:8000/health
```

### –û—à–∏–±–∫–∏ GPU

- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ CUDA —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: `nvidia-smi`
- –î–ª—è Docker –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `--gpus all`
- –ï—Å–ª–∏ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `device: "cpu"`

### –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é

- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, `base` –≤–º–µ—Å—Ç–æ `large-v3`)
- –£–º–µ–Ω—å—à–∏—Ç–µ `num_workers`
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `int8` compute_type –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏